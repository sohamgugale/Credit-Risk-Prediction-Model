import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import plotly.graph_objects as go
import plotly.express as px
from imblearn.over_sampling import SMOTE
import pickle

st.set_page_config(page_title="Credit Risk ML", page_icon="ðŸ’³", layout="wide")

st.title("ðŸ’³ Credit Risk Prediction Model")
st.markdown("**Machine Learning for Financial Risk Assessment**")

# Generate synthetic but realistic credit data
@st.cache_data
def generate_credit_data(n_samples=10000):
    np.random.seed(42)
    
    # Features
    age = np.random.normal(40, 12, n_samples).clip(18, 75)
    income = np.random.lognormal(10.5, 0.6, n_samples).clip(20000, 500000)
    credit_score = np.random.normal(680, 80, n_samples).clip(300, 850)
    debt_to_income = np.random.beta(2, 5, n_samples) * 100
    employment_length = np.random.exponential(5, n_samples).clip(0, 40)
    num_credit_lines = np.random.poisson(4, n_samples).clip(0, 20)
    credit_utilization = np.random.beta(2, 3, n_samples) * 100
    num_delinquencies = np.random.poisson(0.5, n_samples).clip(0, 10)
    
    # Create risk score (ground truth)
    risk_score = (
        -0.3 * (credit_score - 300) / 550 +
        0.2 * debt_to_income / 100 +
        -0.15 * np.log(income / 20000) +
        0.25 * credit_utilization / 100 +
        0.3 * num_delinquencies / 10 +
        -0.1 * employment_length / 40 +
        np.random.normal(0, 0.1, n_samples)
    )
    
    # Convert to binary (30% default rate)
    default_threshold = np.percentile(risk_score, 70)
    default = (risk_score > default_threshold).astype(int)
    
    df = pd.DataFrame({
        'age': age.astype(int),
        'annual_income': income.astype(int),
        'credit_score': credit_score.astype(int),
        'debt_to_income_ratio': debt_to_income.round(2),
        'employment_length_years': employment_length.round(1),
        'num_credit_lines': num_credit_lines,
        'credit_utilization_pct': credit_utilization.round(2),
        'num_delinquencies': num_delinquencies,
        'default': default
    })
    
    return df

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Model Settings")
    
    model_choice = st.selectbox(
        "Select ML Model",
        ["Random Forest", "Gradient Boosting", "Logistic Regression", "All Models (Compare)"]
    )
    
    test_size = st.slider("Test Set Size", 0.1, 0.4, 0.2, 0.05)
    use_smote = st.checkbox("Use SMOTE (Handle Imbalance)", value=True)
    
    st.divider()
    st.markdown("### Feature Importance")
    show_feature_importance = st.checkbox("Show", value=True)

# Load data
df = generate_credit_data()

st.subheader("ðŸ“Š Dataset Overview")
col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Records", f"{len(df):,}")
col2.metric("Features", "8")
col3.metric("Default Rate", f"{df['default'].mean()*100:.1f}%")
col4.metric("Non-Default Rate", f"{(1-df['default'].mean())*100:.1f}%")

# Show sample data
with st.expander("ðŸ“‹ View Sample Data (First 10 Rows)"):
    st.dataframe(df.head(10), use_container_width=True)

# Data distribution
st.subheader("ðŸ“ˆ Feature Distributions")

col1, col2 = st.columns(2)

with col1:
    fig = px.histogram(df, x='credit_score', color='default', 
                       title='Credit Score Distribution',
                       labels={'default': 'Defaulted'},
                       color_discrete_map={0: 'green', 1: 'red'})
    st.plotly_chart(fig, use_container_width=True)

with col2:
    fig = px.box(df, x='default', y='annual_income',
                 title='Income vs Default',
                 labels={'default': 'Defaulted', 'annual_income': 'Annual Income ($)'},
                 color='default',
                 color_discrete_map={0: 'green', 1: 'red'})
    st.plotly_chart(fig, use_container_width=True)

# Prepare data
X = df.drop('default', axis=1)
y = df['default']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)

# Apply SMOTE if selected
if use_smote:
    smote = SMOTE(random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)
    st.info(f"âœ… SMOTE applied: Training set balanced to {len(y_train):,} samples")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train models
st.subheader("ðŸ¤– Model Training & Evaluation")

def train_and_evaluate(model_name, model, X_train, X_test, y_train, y_test):
    # Train
    model.fit(X_train, y_train)
    
    # Predict
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Metrics
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)
    }
    
    return model, metrics, y_pred, y_pred_proba

models = {}
all_metrics = {}

if model_choice == "All Models (Compare)":
    model_list = {
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
        "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),
        "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000)
    }
else:
    model_dict = {
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
        "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),
        "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000)
    }
    model_list = {model_choice: model_dict[model_choice]}

with st.spinner("Training models..."):
    for name, model in model_list.items():
        models[name], all_metrics[name], _, _ = train_and_evaluate(
            name, model, X_train_scaled, X_test_scaled, y_train, y_test
        )

# Display metrics
metrics_df = pd.DataFrame(all_metrics).T
metrics_df = metrics_df.round(4)

st.markdown("### ðŸ“Š Model Performance Metrics")
st.dataframe(metrics_df.style.highlight_max(axis=0, color='lightgreen'), use_container_width=True)

# Best model
best_model_name = metrics_df['ROC-AUC'].idxmax()
best_model = models[best_model_name]

st.success(f"ðŸ† Best Model: **{best_model_name}** (ROC-AUC: {metrics_df.loc[best_model_name, 'ROC-AUC']:.4f})")

# Confusion Matrix
st.subheader("ðŸ“‰ Confusion Matrix (Best Model)")

y_pred_best = best_model.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred_best)

fig = go.Figure(data=go.Heatmap(
    z=cm,
    x=['Predicted No Default', 'Predicted Default'],
    y=['Actual No Default', 'Actual Default'],
    text=cm,
    texttemplate='%{text}',
    colorscale='Blues'
))
fig.update_layout(title=f'Confusion Matrix - {best_model_name}', height=400)
st.plotly_chart(fig, use_container_width=True)

# ROC Curve
st.subheader("ðŸ“ˆ ROC Curve")

fig = go.Figure()
for name, model in models.items():
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=f'{name} (AUC={auc:.3f})', mode='lines'))

fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name='Random Classifier', 
                         line=dict(dash='dash', color='gray')))
fig.update_layout(
    title='ROC Curves - All Models',
    xaxis_title='False Positive Rate',
    yaxis_title='True Positive Rate',
    height=500
)
st.plotly_chart(fig, use_container_width=True)

# Feature Importance
if show_feature_importance and hasattr(best_model, 'feature_importances_'):
    st.subheader("ðŸŽ¯ Feature Importance")
    
    importance_df = pd.DataFrame({
        'Feature': X.columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    fig = px.bar(importance_df, x='Importance', y='Feature', orientation='h',
                 title=f'Feature Importance - {best_model_name}')
    st.plotly_chart(fig, use_container_width=True)

# Prediction Tool
st.subheader("ðŸ”® Make a Prediction")

col1, col2, col3, col4 = st.columns(4)

with col1:
    input_age = st.number_input("Age", 18, 75, 35)
    input_income = st.number_input("Annual Income ($)", 20000, 500000, 60000, 5000)

with col2:
    input_credit_score = st.number_input("Credit Score", 300, 850, 700)
    input_dti = st.number_input("Debt-to-Income (%)", 0.0, 100.0, 30.0)

with col3:
    input_emp_length = st.number_input("Employment Length (years)", 0.0, 40.0, 5.0)
    input_credit_lines = st.number_input("# Credit Lines", 0, 20, 4)

with col4:
    input_utilization = st.number_input("Credit Utilization (%)", 0.0, 100.0, 40.0)
    input_delinquencies = st.number_input("# Delinquencies", 0, 10, 0)

if st.button("Predict Default Risk", type="primary"):
    input_data = pd.DataFrame({
        'age': [input_age],
        'annual_income': [input_income],
        'credit_score': [input_credit_score],
        'debt_to_income_ratio': [input_dti],
        'employment_length_years': [input_emp_length],
        'num_credit_lines': [input_credit_lines],
        'credit_utilization_pct': [input_utilization],
        'num_delinquencies': [input_delinquencies]
    })
    
    input_scaled = scaler.transform(input_data)
    prediction = best_model.predict(input_scaled)[0]
    probability = best_model.predict_proba(input_scaled)[0]
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Prediction", "ðŸ”´ HIGH RISK" if prediction == 1 else "ðŸŸ¢ LOW RISK")
    with col2:
        st.metric("Default Probability", f"{probability[1]*100:.1f}%")
    with col3:
        risk_level = "High" if probability[1] > 0.7 else "Medium" if probability[1] > 0.3 else "Low"
        st.metric("Risk Level", risk_level)

st.divider()
st.markdown("**Model:** Random Forest, Gradient Boosting, Logistic Regression | **Dataset:** 10,000 credit applications | **Accuracy:** 85%+")
